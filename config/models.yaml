# Model configurations
# Detailed settings for all models in the system

models:
  # Memory Server models (embeddings and specialized)
  memory_server:
    embeddings:
      primary:
        name: "all-MiniLM-L6-v2"
        type: "sentence-transformer"
        dimensions: 384
        max_sequence_length: 256
        quantization: "int8"
        
      fallback:
        name: "e5-small-v2"
        type: "sentence-transformer"
        dimensions: 384
        max_sequence_length: 512
        quantization: "fp16"
        
    reranker:
      name: "ms-marco-MiniLM-L-6-v2"
      type: "cross-encoder"
      max_sequence_length: 512
      
  # LLM Server models (main inference models)
  llm_server:
    # Development models
    dev:
      codellama-7b:
        url: "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf"
        size_gb: 4.1
        sha256: "expected_hash_here"
        context_window: 16384
        
      tinyllama-1b:
        url: "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
        size_gb: 0.7
        sha256: "expected_hash_here"
        context_window: 2048
        
    # Productivity models
    productivity:
      llama2-13b:
        url: "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf"
        size_gb: 8.9
        sha256: "expected_hash_here"
        context_window: 4096
        
      mistral-7b:
        url: "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
        size_gb: 4.4
        sha256: "expected_hash_here"
        context_window: 32768
        
    # Academic models
    academic:
      mixtral-8x7b:
        url: "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf"
        size_gb: 30.0
        sha256: "expected_hash_here"
        context_window: 32768
        
      llama2-70b:
        url: "https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf"
        size_gb: 38.9
        sha256: "expected_hash_here"
        context_window: 4096
        
    # General models
    general:
      llama2-7b:
        url: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf"
        size_gb: 4.8
        sha256: "expected_hash_here"
        context_window: 4096
        
      phi-2:
        url: "https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf"
        size_gb: 1.6
        sha256: "expected_hash_here"
        context_window: 2048

# GPU configuration per model type
gpu_allocation:
  auto_detect: true
  force_cpu: false
  layer_split:
    small: 20  # < 3GB models
    medium: 35 # 3-10GB models
    large: 50  # > 10GB models